#!/usr/bin/env python3
"""
E-Ticaret Chatbot Model DeÄŸerlendirme Scripti
Bu script Gemini ve Hugging Face modellerini karÅŸÄ±laÅŸtÄ±rÄ±r.
"""

import pandas as pd
import numpy as np
import sys
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from models.gemini_model import GeminiChatbot
from models.huggingface_model import HuggingFaceChatbot

def load_data(file_path="data/ecommerce_dataset.csv"):
    """Veri setini yÃ¼kle"""
    try:
        data = pd.read_csv(file_path)
        print(f"âœ… Veri seti yÃ¼klendi: {len(data)} Ã¶rnek")
        return data
    except FileNotFoundError:
        print(f"âŒ Veri seti bulunamadÄ±: {file_path}")
        return None

def evaluate_models(data, test_size=0.2, sample_size=50):
    """Modelleri deÄŸerlendir"""
    print("ğŸ”„ Veri seti train/test olarak bÃ¶lÃ¼nÃ¼yor...")
    
    train_data, test_data = train_test_split(
        data, 
        test_size=test_size, 
        random_state=42, 
        stratify=data['intent']
    )
    
    
    test_sample = test_data.head(sample_size) if len(test_data) > sample_size else test_data
    print(f"ğŸ“Š Test seti: {len(test_sample)} Ã¶rnek")
    
    results = {}
    models = {}
    
    try:
        print("ğŸ¤– Gemini modeli baÅŸlatÄ±lÄ±yor...")
        models['Gemini'] = GeminiChatbot()
        print("âœ… Gemini modeli hazÄ±r")
    except Exception as e:
        print(f"âŒ Gemini model hatasÄ±: {e}")
    
    try:
        print("ğŸ¤— Hugging Face modeli baÅŸlatÄ±lÄ±yor...")
        models['Hugging Face'] = HuggingFaceChatbot()
        print("âœ… Hugging Face modeli hazÄ±r")
    except Exception as e:
        print(f"âŒ Hugging Face model hatasÄ±: {e}")
    
    if not models:
        print("âŒ HiÃ§bir model baÅŸlatÄ±lamadÄ±. API anahtarlarÄ±nÄ± kontrol edin.")
        return None
    
    for model_name, model in models.items():
        print(f"\nğŸ”¬ {model_name} modeli deÄŸerlendiriliyor...")
        result = model.evaluate_model(test_sample)
        results[model_name] = result
        
        print(f"ğŸ“ˆ {model_name} SonuÃ§larÄ±:")
        print(f"   Accuracy: {result['accuracy']:.3f}")
        print(f"   Precision: {result['precision']:.3f}")
        print(f"   Recall: {result['recall']:.3f}")
        print(f"   F1 Score: {result['f1_score']:.3f}")
    
    return results, test_sample

def create_comparison_report(results, test_data):
    """KarÅŸÄ±laÅŸtÄ±rma raporu oluÅŸtur"""
    print("\n" + "="*60)
    print("ğŸ“Š MODEL KARÅILAÅTIRMA RAPORU")
    print("="*60)
    
    metrics_df = pd.DataFrame({
        'Model': list(results.keys()),
        'Accuracy': [results[model]['accuracy'] for model in results.keys()],
        'Precision': [results[model]['precision'] for model in results.keys()],
        'Recall': [results[model]['recall'] for model in results.keys()],
        'F1 Score': [results[model]['f1_score'] for model in results.keys()]
    })
    
    print("\nğŸ† PERFORMANS SONUÃ‡LARI:")
    print(metrics_df.to_string(index=False, float_format='%.3f'))
    
    best_model = metrics_df.loc[metrics_df['F1 Score'].idxmax(), 'Model']
    best_f1 = metrics_df.loc[metrics_df['F1 Score'].idxmax(), 'F1 Score']
    
    print(f"\nğŸ¥‡ EN Ä°YÄ° MODEL: {best_model} (F1 Score: {best_f1:.3f})")
    
    print(f"\nğŸ¯ INTENT BAZINDA ANALIZ:")
    unique_intents = test_data['intent'].unique()
    print(f"   Toplam Intent: {len(unique_intents)}")
    print(f"   Intent'ler: {', '.join(unique_intents)}")
    
    return metrics_df

def plot_results(results):
    """SonuÃ§larÄ± gÃ¶rselleÅŸtir"""
    try:
        import matplotlib.pyplot as plt
        
        models = list(results.keys())
        metrics = ['accuracy', 'precision', 'recall', 'f1_score']
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        x = np.arange(len(models))
        width = 0.2
        
        for i, metric in enumerate(metrics):
            values = [results[model][metric] for model in models]
            ax.bar(x + i*width, values, width, label=metric.capitalize())
        
        ax.set_xlabel('Models')
        ax.set_ylabel('Score')
        ax.set_title('Model Performance Comparison')
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(models)
        ax.legend()
        ax.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š Grafik kaydedildi: model_comparison.png")
        
    except ImportError:
        print("âš ï¸ Matplotlib yÃ¼klÃ¼ deÄŸil, grafik oluÅŸturulamadÄ±")

def main():
    """Ana fonksiyon"""
    print("ğŸš€ E-Ticaret Chatbot Model DeÄŸerlendirmesi BaÅŸlÄ±yor...")
    print("="*60)
    
    data = load_data()
    if data is None:
        return
    
    print(f"ğŸ“‹ Veri Seti Bilgileri:")
    print(f"   Toplam Ã–rnek: {len(data)}")
    print(f"   Intent SayÄ±sÄ±: {data['intent'].nunique()}")
    print(f"   Intent DaÄŸÄ±lÄ±mÄ±:")
    for intent, count in data['intent'].value_counts().items():
        print(f"      {intent}: {count}")
    
    results = evaluate_models(data, sample_size=30) 
    
    if results:
        results_dict, test_sample = results
        
        metrics_df = create_comparison_report(results_dict, test_sample)
        
        plot_results(results_dict)
        
        metrics_df.to_csv('model_comparison_results.csv', index=False)
        print("ğŸ’¾ SonuÃ§lar kaydedildi: model_comparison_results.csv")
        
        print("\nâœ… DeÄŸerlendirme tamamlandÄ±!")
    else:
        print("âŒ Model deÄŸerlendirmesi yapÄ±lamadÄ±")

if __name__ == "__main__":
    main() 